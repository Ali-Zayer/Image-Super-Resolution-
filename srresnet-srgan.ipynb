{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import important libraries","metadata":{}},{"cell_type":"code","source":"import PIL  #(Pillow)\nfrom PIL import Image\nimport numpy as np\nfrom numpy import asarray\nimport pandas as pd\nimport os\nimport cv2\nimport imageio\nimport dlib\nimport seaborn as sns\nimport json\nimport random\nimport torchvision\nimport torchvision.transforms.functional as FT\nfrom torchvision import transforms as transforms\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom torch import tensor\nfrom torch.utils import data\nfrom torch.utils.data import Dataset\nimport torch.backends.cudnn as cudnn\nimport math\nfrom math import exp, log10, sqrt\nimport matplotlib.pyplot as plt\nimport time\nimport skimage \nfrom skimage.transform import resize\nfrom skimage.io import imread, imsave\nfrom skimage.color import rgb2gray\nfrom skimage.transform import resize,rescale\nfrom urllib.request import urlopen\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:38:26.910382Z","iopub.execute_input":"2022-05-30T20:38:26.910719Z","iopub.status.idle":"2022-05-30T20:38:30.962137Z","shell.execute_reply.started":"2022-05-30T20:38:26.910616Z","shell.execute_reply":"2022-05-30T20:38:30.961348Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization ","metadata":{}},{"cell_type":"code","source":"num_in_row = 6\nnum_in_col = 3\npath = '../input/div2k-dataset/DIV2K_train_HR/DIV2K_train_HR'\nfull_img = None\nline_img = None\nfiles = [os.path.join(path, file) for file in np.random.choice(os.listdir(path), size=num_in_row * num_in_col)]\nfor i, file in enumerate(files):\n    img = resize(imread(file), (256, 256))\n    if line_img is None:\n        line_img = img\n        continue\n        \n    if (i) % (num_in_row) != 0:\n        line_img = np.concatenate((line_img, img), axis=1)\n    elif full_img is None:\n        full_img = line_img.copy()\n        line_img = img\n    else:\n        full_img = np.concatenate((full_img, line_img), axis=0)\n        line_img = img\nfull_img = np.concatenate((full_img, line_img), axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:38:30.963940Z","iopub.execute_input":"2022-05-30T20:38:30.964616Z","iopub.status.idle":"2022-05-30T20:38:40.072322Z","shell.execute_reply.started":"2022-05-30T20:38:30.964576Z","shell.execute_reply":"2022-05-30T20:38:40.071561Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nplt.imshow(full_img)\nplt.axis('off')\n\nimsave('images.png', full_img)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:38:40.073517Z","iopub.execute_input":"2022-05-30T20:38:40.075564Z","iopub.status.idle":"2022-05-30T20:38:41.721003Z","shell.execute_reply.started":"2022-05-30T20:38:40.075528Z","shell.execute_reply":"2022-05-30T20:38:41.715605Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"files = os.listdir(path)\nlen(files)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:38:41.725607Z","iopub.execute_input":"2022-05-30T20:38:41.725898Z","iopub.status.idle":"2022-05-30T20:38:41.743714Z","shell.execute_reply.started":"2022-05-30T20:38:41.725859Z","shell.execute_reply":"2022-05-30T20:38:41.736663Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def show_imgs(i1,i2,title1,title2):\n    fig=plt.figure(figsize=(15,7))\n    ax = fig.add_subplot(121)\n    plt.gca().set_title(title1)\n    plt.imshow(i1)\n    ax = fig.add_subplot(122)\n    plt.gca().set_title(title2)\n    plt.imshow(i2)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:38:41.744778Z","iopub.execute_input":"2022-05-30T20:38:41.745024Z","iopub.status.idle":"2022-05-30T20:38:41.755602Z","shell.execute_reply.started":"2022-05-30T20:38:41.744989Z","shell.execute_reply":"2022-05-30T20:38:41.754768Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"#  BGR Ð² RGB","metadata":{}},{"cell_type":"code","source":"img = cv2.imread(os.path.join(path, files[29]))\nrgb_img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nshow_imgs(img,rgb_img,'BGR','RGB')\nimg.shape,img.dtype","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:38:41.757181Z","iopub.execute_input":"2022-05-30T20:38:41.757780Z","iopub.status.idle":"2022-05-30T20:38:42.907324Z","shell.execute_reply.started":"2022-05-30T20:38:41.757731Z","shell.execute_reply":"2022-05-30T20:38:42.906606Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"pic = imageio.imread(os.path.join(path, files[792]))\nfig, ax = plt.subplots(nrows = 1, ncols=3, figsize=(15,5))\n\nfor c, ax in zip(range(3), ax):\n    \n    # create zero matrix\n    split_img = np.zeros(pic.shape, dtype=\"uint8\") # 'dtype' by default: 'numpy.float64'\n    \n    # assing each channel \n    split_img[ :, :, c] = pic[ :, :, c]\n    \n    # display each channel\n    ax.imshow(split_img)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:38:42.908751Z","iopub.execute_input":"2022-05-30T20:38:42.909168Z","iopub.status.idle":"2022-05-30T20:38:44.005109Z","shell.execute_reply.started":"2022-05-30T20:38:42.909133Z","shell.execute_reply":"2022-05-30T20:38:44.004491Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Photo was taken in not very good conditions as we reduce the image size (to be around 480x320 pixels).\n# Then we apply operations to correct the image.\n\nimg = cv2.imread(os.path.join(path, files[792]))\nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(7,7))\nplt.imshow(img)\n\ndsize = (320, 480)\nimg=cv2.resize(img, dsize)\nprint(img.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:38:44.006472Z","iopub.execute_input":"2022-05-30T20:38:44.006945Z","iopub.status.idle":"2022-05-30T20:38:44.519122Z","shell.execute_reply.started":"2022-05-30T20:38:44.006908Z","shell.execute_reply":"2022-05-30T20:38:44.518488Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Observe Basic Properties of Image","metadata":{}},{"cell_type":"code","source":"# if we calculate the size of a RGB image, \n# the total size will be counted as height x width x 3\n\nprint('Type of the image : ' , type(pic))\nprint('Shape of the image : {}'.format(pic.shape))\nprint('Image Hight {}'.format(pic.shape[0]))\nprint('Image Width {}'.format(pic.shape[1]))\nprint('Dimension of Image {}'.format(pic.ndim))","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:38:44.520304Z","iopub.execute_input":"2022-05-30T20:38:44.521065Z","iopub.status.idle":"2022-05-30T20:38:44.528238Z","shell.execute_reply.started":"2022-05-30T20:38:44.521028Z","shell.execute_reply":"2022-05-30T20:38:44.527576Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# These values are important to verify since the eight bit color intensity is, can not be outside of the 0 to 255 range.\n\nprint('Image size {}'.format(pic.size))\nprint('Maximum RGB value in this image {}'.format(pic.max()))\nprint('Minimum RGB value in this image {}'.format(pic.min()))","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:38:44.531085Z","iopub.execute_input":"2022-05-30T20:38:44.531821Z","iopub.status.idle":"2022-05-30T20:38:44.550948Z","shell.execute_reply.started":"2022-05-30T20:38:44.531785Z","shell.execute_reply":"2022-05-30T20:38:44.550216Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Experimental Results","metadata":{}},{"cell_type":"code","source":"from torch.utils import data\nfrom torch import tensor\nfrom torchvision import transforms as transforms\nfrom numpy import asarray\n\n\n\nclass HR_Dataset(data.Dataset):\n    'Characterizes a dataset for PyTorch'\n    def __init__(self, path, num_crop_in_image=1, scale=2, crop_size=None, random_crop=True, normalize=True): \n        'Initialization'\n        self.normalize = normalize\n        self.scale = scale\n        self.crop_size = crop_size\n        self.random_crop = random_crop\n        self.files = [os.path.join(path, file) for file in os.listdir(path)] * num_crop_in_image# full pathes to files\n\n    def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.files)\n\n    def crop_img(self, img):\n        if self.random_crop:\n            rand_x = np.random.rand()\n            rand_y = np.random.rand()\n            left_x, top_y, right_x, bottom_y = rand_x * (img.shape[1] - self.crop_size[1]), \\\n            rand_y * (img.shape[0] - self.crop_size[0]), \\\n            rand_x * (img.shape[1] - self.crop_size[1]) + self.crop_size[1], \\\n            rand_y * (img.shape[0] - self.crop_size[0]) + self.crop_size[0]\n        else:\n            center_x, center_y = img.shape[1] / 2, img.shape[0] / 2\n        \n        cropped = img[int(top_y):int(bottom_y),\n                    int(left_x):int(right_x), \n                    :]\n        return cropped\n        \n    def preprocess_image(self, image):\n        if self.crop_size is not None:\n            hr_cropped = self.crop_img(image)\n            lr_cropped = cv2.resize(np.squeeze(hr_cropped), (int(self.crop_size[1] / self.scale), int(self.crop_size[0] / self.scale)))\n\n        if self.normalize:\n            hr_cropped = hr_cropped / 255 # [0, 1]\n            lr_cropped = lr_cropped / 255\n        \n        return tensor(lr_cropped).swapaxes(1,2).swapaxes(0,1), tensor(hr_cropped).swapaxes(1,2).swapaxes(0,1) # transform numpy array into pytorch tensors\n\n\n    def __getitem__(self, index):\n        'Generates one sample of data'\n        \n        file = self.files[index]\n        image = cv2.imread(file)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n         \n        return  self.preprocess_image(image)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:38:44.552045Z","iopub.execute_input":"2022-05-30T20:38:44.552272Z","iopub.status.idle":"2022-05-30T20:38:44.567455Z","shell.execute_reply.started":"2022-05-30T20:38:44.552240Z","shell.execute_reply":"2022-05-30T20:38:44.566719Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"index = 125\nimg = cv2.imread(os.path.join(path, files[index]))\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(10,10))\nplt.imshow(img)\nprint(img.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:38:44.568807Z","iopub.execute_input":"2022-05-30T20:38:44.569260Z","iopub.status.idle":"2022-05-30T20:38:45.266113Z","shell.execute_reply.started":"2022-05-30T20:38:44.569224Z","shell.execute_reply":"2022-05-30T20:38:45.265493Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"data = HR_Dataset(path=path, \n                  num_crop_in_image=4,\n                  scale=4, crop_size=(256,256), \n                  normalize=True)\n\nimg_lr, img_hr = data[125]\n\nprint(img_lr.min(), img_lr.max())\nprint(img_hr.min(), img_hr.max())\nprint(img_lr.shape, img_hr.shape)\n\nplt.figure(figsize=(12,7))\nplt.subplot(121)\nplt.imshow(img_lr.numpy().swapaxes(0,1).swapaxes(1,2))\nplt.subplot(122)\nplt.imshow(img_hr.numpy().swapaxes(0,1).swapaxes(1,2))","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:40:04.610429Z","iopub.execute_input":"2022-05-30T20:40:04.611069Z","iopub.status.idle":"2022-05-30T20:40:05.252734Z","shell.execute_reply.started":"2022-05-30T20:40:04.611025Z","shell.execute_reply":"2022-05-30T20:40:05.251995Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Constants","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Some constants\nrgb_weights = torch.FloatTensor([65.481, 128.553, 24.966]).to(device)\n\n\ndef convert_image(img, source, target):\n    \"\"\"\n    Convert an image from a source format to a target format.\n    :param img: image\n    :param source: source format, one of 'pil' (PIL image), '[0, 1]' or '[-1, 1]' (pixel value ranges)\n    :param target: target format, one of 'pil' (PIL image), '[0, 255]', '[0, 1]', '[-1, 1]' (pixel value ranges),\n                   'y-channel' (luminance channel Y in the YCbCr color format, used to calculate PSNR and SSIM)\n    :return: converted image\n    \"\"\"\n    assert source in {'pil', '[0, 1]', '[-1, 1]'}, \"Cannot convert from source format %s!\" % source\n    assert target in {'pil', '[0, 255]', '[0, 1]', '[-1, 1]',\n                      'y-channel'}, \"Cannot convert to target format %s!\" % target\n\n    # Convert from source to [0, 1]\n    if source == 'pil':\n        img = FT.to_tensor(img)\n\n    elif source == '[0, 1]':\n        pass  # already in [0, 1]\n\n    elif source == '[-1, 1]':\n        img = (img + 1.) / 2.\n\n    # Convert from [0, 1] to target\n    if target == 'pil':\n        img = FT.to_pil_image(img)\n\n    elif target == '[0, 255]':\n        img = 255. * img\n\n    elif target == '[0, 1]':\n        pass  # already in [0, 1]\n\n    elif target == '[-1, 1]':\n        img = 2. * img - 1.\n\n    elif target == 'y-channel':\n        img = torch.matmul(255. * img.permute(0, 2, 3, 1)[:, 4:-4, 4:-4, :], rgb_weights) / 255. + 16.\n\n    return img\n\n\nclass ImageTransforms(object):\n    \"\"\"\n    Image transformation pipeline.\n    \"\"\"\n\n    def __init__(self, split, crop_size, scaling_factor, lr_img_type, hr_img_type):\n        \"\"\"\n        :param split: one of 'train' or 'test'\n        :param crop_size: crop size of HR images\n        :param scaling_factor: LR images will be downsampled from the HR images by this factor\n        :param lr_img_type: the target format for the LR image; see convert_image() above for available formats\n        :param hr_img_type: the target format for the HR image; see convert_image() above for available formats\n        \"\"\"\n        self.split = split.lower()\n        self.crop_size = crop_size\n        self.scaling_factor = scaling_factor\n        self.lr_img_type = lr_img_type\n        self.hr_img_type = hr_img_type\n\n        assert self.split in {'train', 'valid'}\n\n    def __call__(self, img):\n        \"\"\"\n        :param img: a PIL source image from which the HR image will be cropped, and then downsampled to create the LR image\n        :return: LR and HR images in the specified format\n        \"\"\"\n\n        # Crop\n        if self.split == 'train':\n            # Take a random fixed-size crop of the image, which will serve as the high-resolution (HR) image\n            left = random.randint(1, img.width - self.crop_size)\n            top = random.randint(1, img.height - self.crop_size)\n            right = left + self.crop_size\n            bottom = top + self.crop_size\n            hr_img = img.crop((left, top, right, bottom))\n        elif self.split == 'valid':\n            # Take a random fixed-size crop of the image, which will serve as the high-resolution (HR) image\n            left = random.randint(1, img.width - self.crop_size)\n            top = random.randint(1, img.height - self.crop_size)\n            right = left + self.crop_size\n            bottom = top + self.crop_size\n            hr_img = img.crop((left, top, right, bottom))\n        else:\n            # Take the largest possible center-crop of it such that its dimensions are perfectly divisible by the scaling factor\n            x_remainder = img.width % self.scaling_factor\n            y_remainder = img.height % self.scaling_factor\n            left = x_remainder // 2\n            top = y_remainder // 2\n            right = left + (img.width - x_remainder)\n            bottom = top + (img.height - y_remainder)\n            hr_img = img.crop((left, top, right, bottom))\n\n        lr_img = hr_img.resize((int(hr_img.width / self.scaling_factor), int(hr_img.height / self.scaling_factor)),\n                               Image.BICUBIC)\n\n        assert hr_img.width == lr_img.width * self.scaling_factor and hr_img.height == lr_img.height * self.scaling_factor\n\n        lr_img = convert_image(lr_img, source='pil', target=self.lr_img_type)\n        hr_img = convert_image(hr_img, source='pil', target=self.hr_img_type)\n\n        return lr_img, hr_img\n\n\nclass AverageMeter(object):\n    \"\"\"\n    Keeps track of most recent, average, sum, and count of a metric.\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    Clips gradients computed during backpropagation to avoid explosion of gradients.\n    :param optimizer: optimizer with the gradients to be clipped\n    :param grad_clip: clip value\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)\n\n\ndef save_checkpoint(state, filename):\n    \"\"\"\n    Save model checkpoint.\n    :param state: checkpoint contents\n    \"\"\"\n    torch.save(state, filename)\n\n\ndef adjust_learning_rate(optimizer, shrink_factor):\n    \"\"\"\n    Shrinks learning rate by a specified factor.\n    :param optimizer: optimizer whose learning rate must be shrunk.\n    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n    \"\"\"\n\n    print(\"\\nDECAYING learning rate.\")\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = param_group['lr'] * shrink_factor\n    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))# This Python 3 environment comes with many helpful analytics libraries installed","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:40:23.038439Z","iopub.execute_input":"2022-05-30T20:40:23.038735Z","iopub.status.idle":"2022-05-30T20:40:23.063892Z","shell.execute_reply.started":"2022-05-30T20:40:23.038702Z","shell.execute_reply":"2022-05-30T20:40:23.062995Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Now we have to build the class Dataset","metadata":{}},{"cell_type":"code","source":"class SRDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset to be used by a PyTorch DataLoader.\n    \"\"\"\n\n    def __init__(self, data_folder, split, crop_size, scaling_factor, lr_img_type, hr_img_type):\n        \"\"\"\n        :param data_folder: data with images\n        :param split: one of 'train' or 'valid'\n        :param crop_size: crop size of target HR images\n        :param scaling_factor: the input LR images will be downsampled from the target HR images by this factor; the scaling done in the super-resolution\n        :param lr_img_type: the format for the LR image supplied to the model; see convert_image() in utils.py for available formats\n        :param hr_img_type: the format for the HR image supplied to the model; see convert_image() in utils.py for available formats\n        \"\"\"\n\n        self.data_folder = data_folder\n        self.split = split.lower()\n        self.crop_size = int(crop_size)\n        self.scaling_factor = int(scaling_factor)\n        self.lr_img_type = lr_img_type\n        self.hr_img_type = hr_img_type\n\n\n        assert lr_img_type in {'[0, 255]', '[0, 1]', '[-1, 1]'}\n        assert hr_img_type in {'[0, 255]', '[0, 1]', '[-1, 1]'}\n\n        if self.split == 'train':\n            assert self.crop_size % self.scaling_factor == 0, \"Crop dimensions are not perfectly divisible by scaling factor! This will lead to a mismatch in the dimensions of the original HR patches and their super-resolved (SR) versions!\"\n\n        # Read list of image-paths\n        if self.split == 'train':\n            path = os.path.join(data_folder, 'DIV2K_train_HR','DIV2K_train_HR')\n        else:\n            path = os.path.join(data_folder, 'DIV2K_valid_HR', 'DIV2K_valid_HR')\n            \n        self.images = [os.path.join(path, file) for file in os.listdir(path)]\n        \n        # Select the correct set of transforms\n        self.transform = ImageTransforms(split=self.split,\n                                         crop_size=self.crop_size,\n                                         scaling_factor=self.scaling_factor,\n                                         lr_img_type=self.lr_img_type,\n                                         hr_img_type=self.hr_img_type)\n\n    def __getitem__(self, i):\n        \"\"\"\n        This method is required to be defined for use in the PyTorch DataLoader.\n        :param i: index to retrieve\n        :return: the 'i'th pair LR and HR images to be fed into the model\n        \"\"\"\n        # Read image\n        img = Image.open(self.images[i], mode='r')\n        img = img.convert('RGB')\n        if img.width <= 96 or img.height <= 96:\n            print(self.images[i], img.width, img.height)\n        lr_img, hr_img = self.transform(img)\n\n        return lr_img, hr_img\n\n    def __len__(self):\n        \"\"\"\n        This method is required to be defined for use in the PyTorch DataLoader.\n        :return: size of this data (in number of images)\n        \"\"\"\n        return len(self.images)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:40:23.068308Z","iopub.execute_input":"2022-05-30T20:40:23.068524Z","iopub.status.idle":"2022-05-30T20:40:23.081839Z","shell.execute_reply.started":"2022-05-30T20:40:23.068490Z","shell.execute_reply":"2022-05-30T20:40:23.081140Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Convolutional and training model","metadata":{}},{"cell_type":"markdown","source":" # A series of convolutional blocks\n #### The first, third, fifth (and so on) convolutional blocks increase the number of channels but retain image size\n #### The second, fourth, sixth (and so on) convolutional blocks retain the same number of channels but halve image size\n #### The first convolutional block is unique because it does not employ batch normalization","metadata":{}},{"cell_type":"code","source":"\n# first convolutional\n\nclass ConvolutionalBlock(nn.Module):\n    \"\"\"\n    A convolutional block, comprising convolutional, BN, activation layers.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, batch_norm=False, activation=None):\n        \"\"\"\n        :param in_channels: number of input channels\n        :param out_channels: number of output channe;s\n        :param kernel_size: kernel size\n        :param stride: stride\n        :param batch_norm: include a BN layer?\n        :param activation: Type of activation; None if none\n        \"\"\"\n        super(ConvolutionalBlock, self).__init__()\n\n        if activation is not None:\n            activation = activation.lower()\n            assert activation in {'prelu', 'leakyrelu', 'tanh', 'sigmoid'}\n\n        # A container that will hold the layers in this convolutional block\n        layers = list()\n\n        # A convolutional layer\n        layers.append(\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n                      padding=kernel_size // 2))\n\n        # A batch normalization (BN) layer, if wanted\n        if batch_norm is True:\n            layers.append(nn.BatchNorm2d(num_features=out_channels))\n\n        # An activation layer, if wanted\n        if activation == 'prelu':\n            layers.append(nn.PReLU())\n        elif activation == 'leakyrelu':\n            layers.append(nn.LeakyReLU(0.2))\n        elif activation == 'tanh':\n            layers.append(nn.Tanh())\n\n        # Put together the convolutional block as a sequence of the layers in this container\n        self.conv_block = nn.Sequential(*layers)\n\n    def forward(self, input):\n        \"\"\"\n        Forward propagation.\n        :param input: input images, a tensor of size (N, in_channels, w, h)\n        :return: output images, a tensor of size (N, out_channels, w, h)\n        \"\"\"\n        output = self.conv_block(input)  # (N, out_channels, w, h)\n\n        return output\n\n# second convolutional\n\nclass SubPixelConvolutionalBlock(nn.Module):\n    \"\"\"\n    A subpixel convolutional block, comprising convolutional, pixel-shuffle, and PReLU activation layers.\n    \"\"\"\n\n    def __init__(self, kernel_size=3, n_channels=64, scaling_factor=2):\n        \"\"\"\n        :param kernel_size: kernel size of the convolution\n        :param n_channels: number of input and output channels\n        :param scaling_factor: factor to scale input images by (along both dimensions)\n        \"\"\"\n        super(SubPixelConvolutionalBlock, self).__init__()\n\n        # A convolutional layer that increases the number of channels by scaling factor^2, followed by pixel shuffle and PReLU\n        self.conv = nn.Conv2d(in_channels=n_channels, out_channels=n_channels * (scaling_factor ** 2),\n                              kernel_size=kernel_size, padding=kernel_size // 2)\n        # These additional channels are shuffled to form additional pixels, upscaling each dimension by the scaling factor\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=scaling_factor)\n        self.prelu = nn.PReLU()\n\n    def forward(self, input):\n        \"\"\"\n        Forward propagation.\n        :param input: input images, a tensor of size (N, n_channels, w, h)\n        :return: scaled output images, a tensor of size (N, n_channels, w * scaling factor, h * scaling factor)\n        \"\"\"\n        output = self.conv(input)  # (N, n_channels * scaling factor^2, w, h)\n        output = self.pixel_shuffle(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n        output = self.prelu(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n\n        return output\n\n# third convolutional\n\nclass ResidualBlock(nn.Module):\n    \"\"\"\n    A residual block, comprising two convolutional blocks with a residual connection across them.\n    \"\"\"\n\n    def __init__(self, kernel_size=3, n_channels=64):\n        \"\"\"\n        :param kernel_size: kernel size\n        :param n_channels: number of input and output channels (same because the input must be added to the output)\n        \"\"\"\n        super(ResidualBlock, self).__init__()\n\n        # The first convolutional block\n        self.conv_block1 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n                                              batch_norm=True, activation='PReLu')\n\n        # The second convolutional block\n        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n                                              batch_norm=True, activation=None)\n\n    def forward(self, input):\n        \"\"\"\n        Forward propagation.\n        :param input: input images, a tensor of size (N, n_channels, w, h)\n        :return: output images, a tensor of size (N, n_channels, w, h)\n        \"\"\"\n        residual = input  # (N, n_channels, w, h)\n        output = self.conv_block1(input)  # (N, n_channels, w, h)\n        output = self.conv_block2(output)  # (N, n_channels, w, h)\n        output = output + residual  # (N, n_channels, w, h)\n\n        return output\n\n# fourth convolutional\n\nclass SRResNet(nn.Module):\n    \"\"\"\n    The SRResNet, as defined in the paper.\n    \"\"\"\n\n    def __init__(self, large_kernel_size=9, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=4):\n        \"\"\"\n        :param large_kernel_size: kernel size of the first and last convolutions which transform the inputs and outputs\n        :param small_kernel_size: kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n        :param n_channels: number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n        :param n_blocks: number of residual blocks\n        :param scaling_factor: factor to scale input images by (along both dimensions) in the subpixel convolutional block\n        \"\"\"\n        super(SRResNet, self).__init__()\n\n        # Scaling factor must be 2, 4, or 8\n        scaling_factor = int(scaling_factor)\n        assert scaling_factor in {2, 4, 8}, \"The scaling factor must be 2, 4, or 8!\"\n\n        # The first convolutional block\n        self.conv_block1 = ConvolutionalBlock(in_channels=3, out_channels=n_channels, kernel_size=large_kernel_size,\n                                              batch_norm=False, activation='PReLu')\n\n        # A sequence of n_blocks residual blocks, each containing a skip-connection across the block\n        self.residual_blocks = nn.Sequential(\n            *[ResidualBlock(kernel_size=small_kernel_size, n_channels=n_channels) for i in range(n_blocks)])\n\n        # Another convolutional block\n        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels,\n                                              kernel_size=small_kernel_size,\n                                              batch_norm=True, activation=None)\n\n        # Upscaling is done by sub-pixel convolution, with each such block upscaling by a factor of 2\n        n_subpixel_convolution_blocks = int(math.log2(scaling_factor))\n        self.subpixel_convolutional_blocks = nn.Sequential(\n            *[SubPixelConvolutionalBlock(kernel_size=small_kernel_size, n_channels=n_channels, scaling_factor=2) for i\n              in range(n_subpixel_convolution_blocks)])\n\n        # The last convolutional block\n        self.conv_block3 = ConvolutionalBlock(\n            in_channels=n_channels, \n            out_channels=3, \n            kernel_size=large_kernel_size,\n            batch_norm=False, \n            activation='tanh')\n\n    def forward(self, lr_imgs):\n        \"\"\"\n        Forward prop.\n        :param lr_imgs: low-resolution input images, a tensor of size (N, 3, w, h)\n        :return: super-resolution output images, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n        \"\"\"\n        output = self.conv_block1(lr_imgs)  # (N, 3, w, h)\n        residual = output  # (N, n_channels, w, h)\n        output = self.residual_blocks(output)  # (N, n_channels, w, h)\n        output = self.conv_block2(output)  # (N, n_channels, w, h)\n        output = output + residual  # (N, n_channels, w, h)\n        output = self.subpixel_convolutional_blocks(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n        sr_imgs = self.conv_block3(output)  # (N, 3, w * scaling factor, h * scaling factor)\n\n        return sr_imgs\n\n# fifth convolutional\n\nclass Generator(nn.Module):\n    \"\"\"\n    The generator in the SRGAN, as defined in the paper. Architecture identical to the SRResNet.\n    \"\"\"\n\n    def __init__(self, large_kernel_size=9, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=4):\n        \"\"\"\n        :param large_kernel_size: kernel size of the first and last convolutions which transform the inputs and outputs\n        :param small_kernel_size: kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n        :param n_channels: number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n        :param n_blocks: number of residual blocks\n        :param scaling_factor: factor to scale input images by (along both dimensions) in the subpixel convolutional block\n        \"\"\"\n        super(Generator, self).__init__()\n\n        # The generator is simply an SRResNet, as above\n        self.net = SRResNet(large_kernel_size=large_kernel_size, small_kernel_size=small_kernel_size,\n                            n_channels=n_channels, n_blocks=n_blocks, scaling_factor=scaling_factor)\n\n    def initialize_with_srresnet(self, srresnet_checkpoint):\n        \"\"\"\n        Initialize with weights from a trained SRResNet.\n        :param srresnet_checkpoint: checkpoint filepath\n        \"\"\"\n        srresnet = torch.load(srresnet_checkpoint)['model']\n        self.net.load_state_dict(srresnet.state_dict())\n\n        print(\"\\nLoaded weights from pre-trained SRResNet.\\n\")\n\n    def forward(self, lr_imgs):\n        \"\"\"\n        Forward prop.\n        :param lr_imgs: low-resolution input images, a tensor of size (N, 3, w, h)\n        :return: super-resolution output images, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n        \"\"\"\n        sr_imgs = self.net(lr_imgs)  # (N, n_channels, w * scaling factor, h * scaling factor)\n\n        return sr_imgs\n\n# sixth convolutional\n\nclass Discriminator(nn.Module):\n    \"\"\"\n    The discriminator in the SRGAN, as defined in the paper.\n    \"\"\"\n\n    def __init__(self, kernel_size=3, n_channels=64, n_blocks=8, fc_size=1024):\n        \"\"\"\n        :param kernel_size: kernel size in all convolutional blocks\n        :param n_channels: number of output channels in the first convolutional block, after which it is doubled in every 2nd block thereafter\n        :param n_blocks: number of convolutional blocks\n        :param fc_size: size of the first fully connected layer\n        \"\"\"\n        super(Discriminator, self).__init__()\n\n        in_channels = 3\n        conv_blocks = list()\n        for i in range(n_blocks):\n            out_channels = (n_channels if i is 0 else in_channels * 2) if i % 2 is 0 else in_channels\n            conv_blocks.append(\n                ConvolutionalBlock(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n                                   stride=1 if i % 2 is 0 else 2, batch_norm=i is not 0, activation='LeakyReLu'))\n            in_channels = out_channels\n        self.conv_blocks = nn.Sequential(*conv_blocks)\n\n        # An adaptive pool layer that resizes it to a standard size\n        # For the default input size of 96 and 8 convolutional blocks, this will have no effect\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))\n\n        self.fc1 = nn.Linear(out_channels * 6 * 6, fc_size)\n\n        self.leaky_relu = nn.LeakyReLU(0.2)\n\n        self.fc2 = nn.Linear(1024, 1)\n\n        # Don't need a sigmoid layer because the sigmoid operation is performed by PyTorch's nn.BCEWithLogitsLoss()\n\n    def forward(self, imgs):\n        \"\"\"\n        Forward propagation.\n        :param imgs: high-resolution or super-resolution images which must be classified as such, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n        :return: a score (logit) for whether it is a high-resolution image, a tensor of size (N)\n        \"\"\"\n        batch_size = imgs.size(0)\n        output = self.conv_blocks(imgs)\n        output = self.adaptive_pool(output)\n        output = self.fc1(output.view(batch_size, -1))\n        output = self.leaky_relu(output)\n        logit = self.fc2(output)\n\n        return logit\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:40:23.172208Z","iopub.execute_input":"2022-05-30T20:40:23.172416Z","iopub.status.idle":"2022-05-30T20:40:23.208913Z","shell.execute_reply.started":"2022-05-30T20:40:23.172393Z","shell.execute_reply":"2022-05-30T20:40:23.208173Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Class VGG19","metadata":{}},{"cell_type":"code","source":"\nclass TruncatedVGG19(nn.Module):\n    \"\"\"\n    A truncated VGG19 network, such that its output is the 'feature map obtained by the j-th convolution (after activation)\n    before the i-th maxpooling layer within the VGG19 network', as defined in the paper.\n    Used to calculate the MSE loss in this VGG feature-space, i.e. the VGG loss.\n    \"\"\"\n\n    def __init__(self, i, j):\n        \"\"\"\n        :param i: the index i in the definition above\n        :param j: the index j in the definition above\n        \"\"\"\n        super(TruncatedVGG19, self).__init__()\n\n        # Load the pre-trained VGG19 available in torchvision\n        vgg19 = torchvision.models.vgg19(pretrained=True)\n\n        maxpool_counter = 0\n        conv_counter = 0\n        truncate_at = 0\n        # Iterate through the convolutional section (\"features\") of the VGG19\n        for layer in vgg19.features.children():\n            truncate_at += 1\n\n            # Count the number of maxpool layers and the convolutional layers after each maxpool\n            if isinstance(layer, nn.Conv2d):\n                conv_counter += 1\n            if isinstance(layer, nn.MaxPool2d):\n                maxpool_counter += 1\n                conv_counter = 0\n\n            if maxpool_counter == i - 1 and conv_counter == j:\n                break\n\n        assert maxpool_counter == i - 1 and conv_counter == j, \"One or both of i=%d and j=%d are not valid choices for the VGG19!\" % (\n            i, j)\n\n        self.truncated_vgg19 = nn.Sequential(*list(vgg19.features.children())[:truncate_at + 1])\n\n    def forward(self, input):\n        \"\"\"\n        Forward propagation\n        :param input: high-resolution or super-resolution images, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n        :return: the specified VGG19 feature map, a tensor of size (N, feature_map_channels, feature_map_w, feature_map_h)\n        \"\"\"\n        output = self.truncated_vgg19(input)  # (N, feature_map_channels, feature_map_w, feature_map_h)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:40:23.210893Z","iopub.execute_input":"2022-05-30T20:40:23.211161Z","iopub.status.idle":"2022-05-30T20:40:23.222546Z","shell.execute_reply.started":"2022-05-30T20:40:23.211127Z","shell.execute_reply":"2022-05-30T20:40:23.221764Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Metrics  PSNR & SSIM","metadata":{}},{"cell_type":"code","source":"class PSNR:\n    \"\"\"Peak Signal to Noise Ratio\n    img1 and img2 have range [0, 255]\"\"\"\n\n    def __init__(self):\n        self.name = \"PSNR\"\n\n    @staticmethod\n    def __call__(img1, img2):\n        mse = torch.mean((img1 - img2) ** 2)\n        return 20 * torch.log10(1.0 / torch.sqrt(mse))\n\ndef psnr(img1, img2):\n    mse = torch.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return 100\n    pixel_max = 1.0\n    return 20 * log10(pixel_max / sqrt(mse))\n\n\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n    return gauss / gauss.sum()\n\n\ndef create_window(window_size, channel):\n    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n    return window\n\n\ndef _ssim(img1, img2, window, window_size, channel, size_average=True):\n    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1 * mu2\n\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n\n    C1 = 0.01 ** 2\n    C2 = 0.03 ** 2\n\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n\n    if size_average:\n        return ssim_map.mean()\n    else:\n        return ssim_map.mean(1).mean(1).mean(1)\n\n\nclass SSIM(torch.nn.Module):\n    def __init__(self, window_size=11, size_average=True):\n        super(SSIM, self).__init__()\n        self.window_size = window_size\n        self.size_average = size_average\n        self.channel = 1\n        self.window = create_window(window_size, self.channel)\n     \n    def forward(self, img1, img2):\n        (_, channel, _, _) = img1.size()\n\n        if channel == self.channel and self.window.data.type() == img1.data.type():\n            window = self.window\n        else:\n            window = create_window(self.window_size, channel)\n\n            if img1.is_cuda:\n                window = window.cuda(img1.get_device())\n            window = window.type_as(img1)\n\n            self.window = window\n            self.channel = channel\n\n        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n\n\ndef ssim(img1, img2, window_size=11, size_average=True):\n    (_, channel, _, _) = img1.size()\n    window = create_window(window_size, channel)\n\n    if img1.is_cuda:\n        window = window.cuda(img1.get_device())\n    window = window.type_as(img1)\n\n    return _ssim(img1, img2, window, window_size, channel, size_average)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:40:23.224004Z","iopub.execute_input":"2022-05-30T20:40:23.224405Z","iopub.status.idle":"2022-05-30T20:40:23.245894Z","shell.execute_reply.started":"2022-05-30T20:40:23.224369Z","shell.execute_reply":"2022-05-30T20:40:23.245233Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Execution","metadata":{}},{"cell_type":"code","source":"# Data parameters\ndata_folder = '../input/div2k-dataset/'  # folder with JSON data files\ncrop_size = 96  # crop size of target HR images\nscaling_factor = 4  # the scaling factor for the generator; the input LR images will be downsampled from the target HR images by this factor\n\n# Generator parameters\nlarge_kernel_size_g = 9  # kernel size of the first and last convolutions which transform the inputs and outputs\nsmall_kernel_size_g = 3  # kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\nn_channels_g = 128  # number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\nn_blocks_g = 32  # number of residual blocks\nsrresnet_checkpoint = '../input/weightsresnet/checkpoint_srresnet.pth.tar'  # filepath of the trained SRResNet checkpoint used for initialization\n                  \n# Discriminator parameters\nkernel_size_d = 3  # kernel size in all convolutional blocks\nn_channels_d = 64  # number of output channels in the first convolutional block, after which it is doubled in every 2nd block thereafter\nn_blocks_d = 8  # number of convolutional blocks\nfc_size_d = 1024  # size of the first fully connected layer\n\n# Learning parameters\n\nif os.path.exists('./checkpoint_srgan.pth.tar'):\n    checkpoint = './checkpoint_srgan.pth.tar'\n#     srresnet_checkpoint = None\nelif os.path.exists('../input/sr-gan/checkpoint_srgan.pth.tar'):\n    checkpoint = '../input/sr-gan/checkpoint_srgan.pth.tar'\n#     srresnet_checkpoint = None\nelse:\n    checkpoint = None  # path to model (SRGAN) checkpoint, None if none\n# checkpoint = None\n\nbatch_size = 64  # batch size\n\nstart_epoch = 0  # start at this epoch\nepochs = 1260    # # number of training iterations\n\niterations = 2e5  # 200000 number of training iterations\nworkers = 4  # number of workers for loading data in the DataLoader\nvgg19_i = 5  # the index i in the definition for VGG loss; see paper or models.py\nvgg19_j = 4  # the index j in the definition for VGG loss; see paper or models.py\nbeta = 1e-3  # the coefficient to weight the adversarial loss in the perceptual loss\nprint_freq = 500  # print training status once every __ batches\nlr = 1e-4  # learning rate\ngrad_clip = None  # clip if gradients are exploding\n\n# Default device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ncudnn.benchmark = True\n\nloss_curve_train = []\nloss_curve_valid = []\n\n\n\ndef main():\n    \"\"\"\n    Training.\n    \"\"\"\n    global start_epoch, epoch, checkpoint, srresnet_checkpoint\n\n    # Initialize model or load checkpoint\n    if checkpoint is None:\n        # Generator\n        generator = Generator(large_kernel_size=large_kernel_size_g,\n                              small_kernel_size=small_kernel_size_g,\n                              n_channels=n_channels_g,\n                              n_blocks=n_blocks_g,\n                              scaling_factor=scaling_factor)\n\n        # Initialize generator network with pretrained SRResNet\n        generator.initialize_with_srresnet(srresnet_checkpoint=srresnet_checkpoint)\n\n        # Initialize generator's optimizer\n        optimizer_g = torch.optim.Adam(params=filter(lambda p: p.requires_grad, generator.parameters()),\n                                       lr=lr)\n\n        # Discriminator\n        discriminator = Discriminator(kernel_size=kernel_size_d,\n                                      n_channels=n_channels_d,\n                                      n_blocks=n_blocks_d,\n                                      fc_size=fc_size_d)\n\n        # Initialize discriminator's optimizer\n        optimizer_d = torch.optim.Adam(params=filter(lambda p: p.requires_grad, discriminator.parameters()),\n                                       lr=lr)\n\n    else:\n        checkpoint = torch.load(checkpoint)\n        start_epoch = checkpoint['epoch'] + 1\n        generator = checkpoint['generator']\n        discriminator = checkpoint['discriminator']\n        optimizer_g = checkpoint['optimizer_g']\n        optimizer_d = checkpoint['optimizer_d']\n        print(\"\\nLoaded checkpoint from epoch %d.\\n\" % (checkpoint['epoch'] + 1))\n\n    # Truncated VGG19 network to be used in the loss calculation\n    truncated_vgg19 = TruncatedVGG19(i=vgg19_i, j=vgg19_j)\n    truncated_vgg19.eval()\n\n    # Loss functions\n    criterion = nn.L1Loss()\n    content_loss_criterion = nn.MSELoss()\n    adversarial_loss_criterion = nn.BCEWithLogitsLoss()\n\n    # Move to default device\n    generator = generator.to(device)\n    discriminator = discriminator.to(device)\n    truncated_vgg19 = truncated_vgg19.to(device)\n    content_loss_criterion = content_loss_criterion.to(device)\n    adversarial_loss_criterion = adversarial_loss_criterion.to(device)\n\n    # Custom dataloaders\n    train_dataset = SRDataset(data_folder,\n                              split='train',\n                              crop_size=crop_size,\n                              scaling_factor=scaling_factor,\n                              lr_img_type='[-1, 1]',\n                              hr_img_type='[-1, 1]')\n    \n    valid_dataset = SRDataset(data_folder,\n                              split='valid',\n                              crop_size=crop_size,\n                              scaling_factor=scaling_factor,\n                              lr_img_type='[-1, 1]',\n                              hr_img_type='[-1, 1]')\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=workers,\n        pin_memory=True)\n    \n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=workers,\n        pin_memory=True)\n\n    # Epochs\n    for epoch in range(start_epoch, epochs):\n\n        # At the halfway point, reduce learning rate to a tenth\n        if epoch == int((iterations / 2) // len(train_loader) + 1):\n            adjust_learning_rate(optimizer_g, 0.1)\n            adjust_learning_rate(optimizer_d, 0.1)\n\n        # One epoch's training\n        train_loss = train(\n            train_loader=train_loader,\n            generator=generator,\n            discriminator=discriminator,\n            truncated_vgg19=truncated_vgg19,\n            content_loss_criterion=content_loss_criterion,\n            adversarial_loss_criterion=adversarial_loss_criterion,\n            criterion=criterion,\n            optimizer_g=optimizer_g,\n            optimizer_d=optimizer_d,\n            epoch=epoch)\n        \n        valid_loss = valid(\n            valid_loader=valid_loader,\n            model=generator,\n            criterion=criterion)\n        \n        print('Epoch: [{0}]----'\n              'Train Average Loss: {train_loss.avg:.4f}---- '\n              'Valid Average Loss: {valid_loss.avg:.4f} '\n              .format(epoch,\n                     train_loss=train_loss,\n                     valid_loss=valid_loss))\n        \n        loss_curve_train.append(train_loss.avg)\n        loss_curve_valid.append(valid_loss.avg)\n        # Save checkpoint\n        torch.save({'epoch': epoch,\n                    'generator': generator,\n                    'discriminator': discriminator,\n                    'optimizer_g': optimizer_g,\n                    'optimizer_d': optimizer_d},\n                   'checkpoint_srgan.pth.tar')\n\n\ndef train(train_loader, \n          generator, \n          discriminator, \n          truncated_vgg19, \n          content_loss_criterion, \n          adversarial_loss_criterion,\n          criterion,\n          optimizer_g, \n          optimizer_d, \n          epoch):\n    \"\"\"\n    One epoch's training.\n    :param train_loader: train dataloader\n    :param generator: generator\n    :param discriminator: discriminator\n    :param truncated_vgg19: truncated VGG19 network\n    :param content_loss_criterion: content loss function (Mean Squared-Error loss)\n    :param adversarial_loss_criterion: adversarial loss function (Binary Cross-Entropy loss)\n    :param optimizer_g: optimizer for the generator\n    :param optimizer_d: optimizer for the discriminator\n    :param epoch: epoch number\n    \"\"\"\n    # Set to train mode\n    generator.train()\n    discriminator.train()  # training mode enables batch normalization\n\n    batch_time = AverageMeter()  # forward prop. + back prop. time\n    data_time = AverageMeter()  # data loading time\n    losses_c = AverageMeter()  # content loss\n    losses_a = AverageMeter()  # adversarial loss in the generator\n    losses_d = AverageMeter()  # adversarial loss in the discriminator\n    losses = AverageMeter()\n\n\n\n    # Batches\n    for i, (lr_imgs, hr_imgs) in enumerate(train_loader):\n\n        # Move to default device\n        lr_imgs = lr_imgs.to(device)  # (batch_size (N), 3, 24, 24), imagenet-normed\n        hr_imgs = hr_imgs.to(device)  # (batch_size (N), 3, 96, 96), imagenet-normed\n\n        # GENERATOR UPDATE\n\n        # Generate\n        sr_imgs = generator(lr_imgs)  # (N, 3, 96, 96), in [-1, 1]\n\n        # Calculate VGG feature maps for the super-resolved (SR) and high resolution (HR) images\n        sr_imgs_in_vgg_space = truncated_vgg19(sr_imgs)\n        hr_imgs_in_vgg_space = truncated_vgg19(hr_imgs).detach()  # detached because they're constant, targets\n\n        # Discriminate super-resolved (SR) images\n        sr_discriminated = discriminator(sr_imgs)  # (N)\n\n        # Calculate the Perceptual loss\n        content_loss = content_loss_criterion(sr_imgs_in_vgg_space, hr_imgs_in_vgg_space)\n        adversarial_loss = adversarial_loss_criterion(sr_discriminated, torch.ones_like(sr_discriminated))\n        perceptual_loss = content_loss + beta * adversarial_loss\n\n        # Back-prop.\n        optimizer_g.zero_grad()\n        perceptual_loss.backward()\n\n        # Clip gradients, if necessary\n        if grad_clip is not None:\n            clip_gradient(optimizer_g, grad_clip)\n\n        # Update generator\n        optimizer_g.step()\n\n        # Keep track of loss\n        losses_c.update(content_loss.item(), lr_imgs.size(0))\n        losses_a.update(adversarial_loss.item(), lr_imgs.size(0))\n\n        # DISCRIMINATOR UPDATE\n\n        # Discriminate super-resolution (SR) and high-resolution (HR) images\n        hr_discriminated = discriminator(hr_imgs)\n        sr_discriminated = discriminator(sr_imgs.detach())\n\n        # Binary Cross-Entropy loss\n        adversarial_loss = adversarial_loss_criterion(sr_discriminated, torch.zeros_like(sr_discriminated)) + \\\n                           adversarial_loss_criterion(hr_discriminated, torch.ones_like(hr_discriminated))\n\n        # Back-prop.\n        optimizer_d.zero_grad()\n        adversarial_loss.backward()\n\n        # Clip gradients, if necessary\n        if grad_clip is not None:\n            clip_gradient(optimizer_d, grad_clip)\n\n        # Update discriminator\n        optimizer_d.step()\n\n        # Keep track of loss\n        losses_d.update(adversarial_loss.item(), hr_imgs.size(0))\n        \n        losses.update(criterion(sr_imgs, hr_imgs).item(), lr_imgs.size(0))\n\n    del lr_imgs, hr_imgs, sr_imgs, hr_imgs_in_vgg_space, sr_imgs_in_vgg_space, hr_discriminated, sr_discriminated  # free some memory since their histories may be stored\n    \n    return losses\n\ndef valid(valid_loader, model, criterion):\n    \"\"\"\n    One epoch's training.\n    :param train_loader: DataLoader for training data\n    :param model: model\n    :param criterion: content loss function (Mean Squared-Error loss)\n    :param optimizer: optimizer\n    :param epoch: epoch number\n    \"\"\"\n    model.eval() \n\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    for i, (lr_imgs, hr_imgs) in enumerate(valid_loader):\n\n        # Move to default device\n        lr_imgs = lr_imgs.to(device)  # (batch_size (N), 3, 24, 24), imagenet-normed\n        hr_imgs = hr_imgs.to(device)  # (batch_size (N), 3, 96, 96), in [-1, 1]\n\n        # Forward prop.\n        sr_imgs = model(lr_imgs)  # (N, 3, 96, 96), in [-1, 1]\n\n        # Loss\n        loss = criterion(sr_imgs, hr_imgs)  # scalar\n\n        # Keep track of loss\n        losses.update(loss.item(), lr_imgs.size(0))\n\n\n    del lr_imgs, hr_imgs, sr_imgs  # free some memory since their histories may be stored\n    return losses\n\nif __name__ == '__main__':\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:40:23.306528Z","iopub.execute_input":"2022-05-30T20:40:23.306733Z","iopub.status.idle":"2022-05-30T20:51:05.527865Z","shell.execute_reply.started":"2022-05-30T20:40:23.306707Z","shell.execute_reply":"2022-05-30T20:51:05.526772Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(loss_curve_train, label='train')\nplt.plot(loss_curve_valid, label='valid')\nplt.legend()\nplt.xlabel('number of training epochs')\nplt.ylabel('pixel loss')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:51:05.534524Z","iopub.execute_input":"2022-05-30T20:51:05.536950Z","iopub.status.idle":"2022-05-30T20:51:05.768355Z","shell.execute_reply.started":"2022-05-30T20:51:05.536899Z","shell.execute_reply":"2022-05-30T20:51:05.767562Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"crop_size = 4*96  # crop size of target HR images\nscaling_factor = 4\ntrain_dataset = SRDataset('../input/div2k-dataset',\n                          split='train',\n                          crop_size=crop_size,\n                          scaling_factor=scaling_factor,\n                          lr_img_type='[-1, 1]',\n                          hr_img_type='[-1, 1]')\n\n\nfor i in range(0,10):\n    img_lr, img_hr = train_dataset[8+i]\n    plt.figure(figsize=(20,15))\n    plt.subplot(131)\n    plt.imshow((img_lr.swapaxes(0,1).swapaxes(1,2) + 1) * 0.5)\n    plt.title('LR image')\n    plt.subplot(132)\n    plt.imshow((img_hr.swapaxes(0,1).swapaxes(1,2) + 1) * 0.5)\n    plt.title('HR image')\n\n    checkpoint = torch.load('checkpoint_srgan.pth.tar')\n    generator = checkpoint['generator']\n    imr_pred = generator(torch.tensor(img_lr).unsqueeze(0).to(device))\n\n    plt.subplot(133)\n    plt.imshow((imr_pred.detach().cpu().squeeze().numpy().swapaxes(0,1).swapaxes(1,2) + 1) * 0.5)\n    plt.title('Predicted image')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:51:05.769761Z","iopub.execute_input":"2022-05-30T20:51:05.770075Z","iopub.status.idle":"2022-05-30T20:51:19.080113Z","shell.execute_reply.started":"2022-05-30T20:51:05.770030Z","shell.execute_reply":"2022-05-30T20:51:19.079499Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Apply the results to some image","metadata":{}},{"cell_type":"code","source":"img = imread('https://www.popsci.com/uploads/2019/01/07/GMY7X5OKPSBGSTSFEZXDEYW6JU-1024x1024.jpg').astype(float) / 255","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:51:19.081482Z","iopub.execute_input":"2022-05-30T20:51:19.082121Z","iopub.status.idle":"2022-05-30T20:51:19.535899Z","shell.execute_reply.started":"2022-05-30T20:51:19.082084Z","shell.execute_reply":"2022-05-30T20:51:19.535125Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"\nimg_lr = resize(img,(int(img.shape[0]/4), int(img.shape[1]/4)))\nplt.figure(figsize=(30,30))\nplt.subplot(311)\nplt.imshow(img_lr)\nplt.title('Low resolution image')\nplt.axis('off')\n\n\nplt.subplot(312)\nplt.imshow(img)\nplt.title('Original image')\nplt.axis('off')\n\ncheckpoint = torch.load('checkpoint_srgan.pth.tar')\ngenerator = checkpoint['generator']\nimr_pred = generator(torch.tensor(img_lr).swapaxes(1,2).swapaxes(0,1).unsqueeze(0).to(device).float())\n\nplt.subplot(313)\nplt.imshow(imr_pred.detach().cpu().squeeze().numpy().swapaxes(0,1).swapaxes(1,2))\nplt.title('Generated image')\nplt.axis('off')\n\n# plt.savefig('earth.png', dpi=300, bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:51:19.537830Z","iopub.execute_input":"2022-05-30T20:51:19.538088Z","iopub.status.idle":"2022-05-30T20:51:22.663499Z","shell.execute_reply.started":"2022-05-30T20:51:19.538054Z","shell.execute_reply":"2022-05-30T20:51:22.662894Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"img_lr = resize(img,(int(img.shape[0]/4), int(img.shape[1]/4)))\nplt.figure(figsize=(20,15))\nplt.subplot(131)\nplt.imshow(img_lr)\nplt.title('Low resolution image')\nplt.axis('off')\n\nplt.subplot(132)\nplt.imshow(img)\nplt.title('Original image')\nplt.axis('off')\n\ncheckpoint = torch.load('checkpoint_srgan.pth.tar')\ngenerator = checkpoint['generator']\nimr_pred = generator(torch.tensor(img_lr).swapaxes(1,2).swapaxes(0,1).unsqueeze(0).to(device).float())\n\nplt.subplot(133)\nplt.imshow(imr_pred.detach().cpu().squeeze().numpy().swapaxes(0,1).swapaxes(1,2))\nplt.title('Generated image')\nplt.axis('off')\n\n# plt.savefig('earth.png', dpi=300, bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:51:22.664684Z","iopub.execute_input":"2022-05-30T20:51:22.665031Z","iopub.status.idle":"2022-05-30T20:51:24.131535Z","shell.execute_reply.started":"2022-05-30T20:51:22.665000Z","shell.execute_reply":"2022-05-30T20:51:24.130893Z"},"trusted":true},"execution_count":38,"outputs":[]}]}